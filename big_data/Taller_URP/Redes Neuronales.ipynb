{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/subprocess32.py:146: RuntimeWarning: The _posixsubprocess module is not being used. Child process reliability may suffer if your program uses threads.\n",
      "  \"program uses threads.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2/\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Redes_Neuronales').getOrCreate()\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt, numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-----+\n",
      "|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|label|\n",
      "+------------+-----------+------------+-----------+-------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|    1|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|    1|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|    1|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|    1|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|    1|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|    1|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|    1|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|    1|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|    1|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|    1|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|    1|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|    1|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|    1|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|    1|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|    1|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|    1|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|    1|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|    1|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|    1|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|    1|\n",
      "+------------+-----------+------------+-----------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainning = spark.read.csv(\"/user/epinedac/datasets/iris\", header=True, nullValue=\"?\", inferSchema=True)\n",
    "trainning.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\"], outputCol=\"features\")\n",
    "assem_data = assembler.transform(trainning)\n",
    "\n",
    "train_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "train_scaler_model = train_scaler.fit(assem_data)\n",
    "scaled_data_train = train_scaler_model.transform(assem_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = scaled_data_train.randomSplit([0.7, 0.3], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# Creando modelo asignando las capas \n",
    "\n",
    "layers = [4, 1, 4]\n",
    "\n",
    "trainerassem = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "trainedModel = trainerassem.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3254788823165669,-8.461648570404304,14.003872022704536,4.898695552907848,-1.7813087757716815,-77.3949743183719,10.599971536648164,33.02190120410698,33.86170247977203,-76.83272336658241,33.31416843077716,22.042447563851976,21.120414098429286]\n"
     ]
    }
   ],
   "source": [
    "print trainedModel.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.625\n",
      "F1 Score = 0.509615384615\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "result = trainedModel.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "evaluatorF1 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "print(\"Accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))\n",
    "print(\"F1 Score = \" + str(evaluatorF1.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codigo para obtener la matriz de confusi√≥n\n",
    "result.createOrReplaceTempView(\"tabla_rna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+\n",
      "|real|prediction|cantidad|\n",
      "+----+----------+--------+\n",
      "|   1|       1.0|      13|\n",
      "|   2|       2.0|      12|\n",
      "|   3|       2.0|      15|\n",
      "+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select label as real, prediction, count(1) as cantidad\n",
    "              from tabla_rna\n",
    "              group by label, prediction\n",
    "              order by 1, 2 asc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
